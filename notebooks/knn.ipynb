{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis con KNN\n",
    "## Clasificador en C++ üí™üí™\n",
    "Vamos a probar a nuestro bichito\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir los path al ejecutable de python 3.6 y sus librer√≠as,\n",
    "de acuerdo al virtual env que est√©n corriendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: build: File exists\n",
      "-- The C compiler identification is AppleClang 10.0.0.10001145\n",
      "-- The CXX compiler identification is AppleClang 10.0.0.10001145\n",
      "-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc\n",
      "-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++\n",
      "-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "Release mode\n",
      "-- Found PythonInterp: /anaconda3/envs/tp2/bin/python (found version \"3.6.5\") \n",
      "-- Found PythonLibs: /anaconda3/envs/tp2/lib/libpython3.6m.dylib\n",
      "-- pybind11 v2.3.dev0\n",
      "-- Performing Test HAS_FLTO\n",
      "-- Performing Test HAS_FLTO - Success\n",
      "-- LTO enabled\n",
      "CMAKE_INSTALL_PREFIX=/Users/vpomsztein/Documents/PrivateRepositories/SentimentAnalysisMetNum\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /Users/vpomsztein/Documents/PrivateRepositories/SentimentAnalysisMetNum/build\n",
      "\u001b[35m\u001b[1mScanning dependencies of target sentiment\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/sentiment.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/knn.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/pca.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/sentiment.dir/src/eigen.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module sentiment.cpython-36m-darwin.so\u001b[0m\n",
      "[100%] Built target sentiment\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"Release\"\n",
      "-- Installing: /Users/vpomsztein/Documents/PrivateRepositories/SentimentAnalysisMetNum/notebooks/sentiment.cpython-36m-darwin.so\n"
     ]
    }
   ],
   "source": [
    "!cd .. && git submodule init\n",
    "!cd .. && git submodule update\n",
    "!cd .. && mkdir build\n",
    "!cd ../build/ && rm -rf *\n",
    "!cd ../build && cmake \\\n",
    "  -DPYTHON_EXECUTABLE=\"$(which python)\" \\\n",
    "  -DCMAKE_BUILD_TYPE=Release ..\n",
    "!cd ../build && make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vpomsztein/Documents/PrivateRepositories/SentimentAnalysisMetNum/notebooks\n",
      "Python 3.6.5 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "# Verifico la correcta instalaci√≥n. Si no falla el import est√° OK\n",
    "!pwd\n",
    "!python --version\n",
    "import sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x ./._imdb_small.csv\n",
      "x imdb_small.csv\n",
      "Cantidad de documentos: 12500\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "\n",
    "!cd ../data && tar -xvf *.tgz\n",
    "\n",
    "df = pd.read_csv(\"../data/imdb_small.csv\", index_col=0)\n",
    "\n",
    "print(\"Cantidad de documentos: {}\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>12469</td>\n",
       "      <td>2</td>\n",
       "      <td>12085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>test</td>\n",
       "      <td>By far this has to be one of the worst movies ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>4182_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6275</td>\n",
       "      <td>2</td>\n",
       "      <td>6322</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                             review  label  \\\n",
       "count   12500                                              12500  12500   \n",
       "unique      2                                              12469      2   \n",
       "top      test  By far this has to be one of the worst movies ...    neg   \n",
       "freq     6275                                                  2   6322   \n",
       "\n",
       "               file  \n",
       "count         12500  \n",
       "unique        12085  \n",
       "top     4182_10.txt  \n",
       "freq              2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de instancias de entrenamiento = 6225\n",
      "Cantidad de instancias de test = 6275\n"
     ]
    }
   ],
   "source": [
    "text_train = df[df.type == 'train'][\"review\"]\n",
    "label_train = df[df.type == 'train'][\"label\"]\n",
    "\n",
    "text_test = df[df.type == 'test'][\"review\"]\n",
    "label_test = df[df.type == 'test'][\"label\"]\n",
    "\n",
    "#descomentar esto si se quiere tener un dataset m√°s chico que los 6.000 totales\n",
    "#text_train = text_train[:100]\n",
    "#label_train = label_train[:100]\n",
    "\n",
    "#text_test = text_test[:100]\n",
    "#label_test = label_test[:100]\n",
    "\n",
    "print(\"Cantidad de instancias de entrenamiento = {}\".format(len(text_train)))\n",
    "print(\"Cantidad de instancias de test = {}\".format(len(text_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance : 0.49493975903614457 pos 0.5050602409638554 neg\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "print(\"Class balance : {} pos {} neg\".format(\n",
    "    (label_train == 'pos').sum() / label_train.shape[0], \n",
    "    (label_train == 'neg').sum() / label_train.shape[0]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.85, min_df=0.01, max_features=50000)\n",
    "\n",
    "vectorizer.fit(text_train)\n",
    "\n",
    "X_train, y_train = vectorizer.transform(text_train), (label_train == 'pos').values\n",
    "X_test, y_test = vectorizer.transform(text_test), (label_test == 'pos').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! You're a good boy üê∂\n"
     ]
    }
   ],
   "source": [
    "import calendar;\n",
    "import time;\n",
    "\n",
    "def f1score(prec, rec):\n",
    "    f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def getMetrics(predictions, actualValues):\n",
    "    tp = np.sum( np.logical_and(predictions == 1, actualValues == 1) )\n",
    "    tn = np.sum( np.logical_and(predictions == 0, actualValues == 0) )\n",
    "    fp = np.sum( np.logical_and(predictions == 1, actualValues == 0) )\n",
    "    fn = np.sum( np.logical_and(predictions == 0, actualValues == 1) )\n",
    "\n",
    "    acc = (tp + tn) / (tp+tn+fp+fn)\n",
    "    if (tp + fp) == 0:\n",
    "        #esto es que nunca predije que algo era positivo, o sea que nunca le pifi√©\n",
    "        prec = 1\n",
    "    else:\n",
    "        prec = tp / (tp + fp)\n",
    "    if (tp + fn) == 0:\n",
    "        #esto es que no hab√≠a positivos reales, o sea que \"los agarr√© a todos\"\n",
    "        rec = 1\n",
    "    else:\n",
    "        rec = tp / (tp + fn)\n",
    "    return acc, prec, rec\n",
    "\n",
    "def saveData(filename, data):\n",
    "    ts = calendar.timegm(time.gmtime())\n",
    "    np.savetxt(\"{}_{}.csv\".format(ts, filename), data, delimiter=\",\")\n",
    "    \n",
    "print(\"Done! You're a good boy üê∂\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Esta celda puede ser ejecutada s√≥lo cuando se quiere probar PCA\n",
    "pca = sentiment.PCA(50)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Idem anterior!\n",
    "\n",
    "pca_x_train = pca.transform(X_train)\n",
    "pca_x_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Descomentar la implementaci√≥n deseada:\n",
    "# Uncomment this for KNN only (PCA disabled):\n",
    "clf = sentiment.KNNClassifier(100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test[:500])\n",
    "acc = accuracy_score(y_test[:500], y_pred[:500])\n",
    "\n",
    "# Uncomment this for PCA enabled:\n",
    "#clf = sentiment.KNNClassifier(550)\n",
    "#clf.fit(pca_x_train, y_train)\n",
    "\n",
    "#y_pred = clf.predict(pca_x_test)\n",
    "\n",
    "#acc, prec, rec = getMetrics(y_pred, y_test)\n",
    "#f1 = f1score(prec, rec)\n",
    "\n",
    "print(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==================== Experimento 1 ========================\n",
    "## An√°lisis de calidad de resultados en base a k y alpha\n",
    "\n",
    "## Analizo la variaci√≥n de las m√©tricas en funcion de k\n",
    "\n",
    "K_vals = np.arange(0,1000,50)\n",
    "K_vals[0] = 1\n",
    "mets = np.zeros((len(K_vals), 3))\n",
    "               \n",
    "for i,k in enumerate(K_vals):\n",
    "#     pca = sentiment.PCA(a)\n",
    "#     pca.fit(X_train[:1000])\n",
    "#     Xtc_train = pca.transform(X_train[:1000])\n",
    "#     Xtc_test = pca.transform(X_test[:1000])\n",
    "    \n",
    "    clf = sentiment.KNNClassifier(25)\n",
    "    clf.fit(Xtc_train, y_train[:1000])\n",
    "    preds = clf.predict(Xtc_test)\n",
    "    acc, prec, rec = getMetrics(preds[:1000], y_test[:1000])\n",
    "    f1 = f1score(prec, rec)\n",
    "    mets[i] = [acc, prec, rec, f1]\n",
    "    print(\"Finished {}\".format(a))\n",
    "    \n",
    "plt.plot(K_vals,mets[:,0], 'b.-')\n",
    "plt.plot(K_vals,mets[:,1], 'r.-')\n",
    "plt.plot(K_vals,mets[:,2], 'g.-')\n",
    "plt.plot(K_vals,mets[:,3], 'k.-')\n",
    "plt.title(\"Metricas en funcion de Alpha\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"%\")\n",
    "plt.axis([1, K_vals[-1], 0.4, 1])\n",
    "plt.gca().legend(('Acc','Prec','Recall','F1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analizo la variaci√≥n de las m√©tricas en funcion de alpha para K fijo\n",
    "\n",
    "alphas = np.arange(0,500,50)\n",
    "alphas[0] = 1\n",
    "K_vals_fixes = np.array([50,550,1100])\n",
    "\n",
    "dmets = { i : np.zeros((len(alphas), 3)) for i in K_vals_fixes}\n",
    "dmets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning alpha metric calculations\")\n",
    "\n",
    "for i,a in enumerate(alphas):\n",
    "    \n",
    "    pca = sentiment.PCA(a)\n",
    "    pca.fit(X_train)\n",
    "    Xtc_train = pca.transform(X_train)\n",
    "    Xtc_test = pca.transform(X_test)\n",
    "    print(\"Finished training PCA {}\".format(a))\n",
    "\n",
    "    for j, k in enumerate(K_vals_fixes):\n",
    "        clf = sentiment.KNNClassifier(k)\n",
    "        clf.fit(Xtc_train, y_train)\n",
    "        preds = clf.predict(Xtc_test)\n",
    "        acc, prec, rec = getMetrics(preds, y_test)\n",
    "        f1 = f1score(prec,rec)\n",
    "        mets = dmets[k]\n",
    "        mets[i] = [acc, prec, rec, f1]\n",
    "        print(\"Finished {}, alpha = {}\".format(k, a))\n",
    "\n",
    "\n",
    "## Metricas para PCA:\n",
    "\n",
    "line_styles = ['b.-', 'r.-', 'g.-', 'k.-', 'y.-', 'm.-']\n",
    "for j,k in enumerate(K_vals_fixes):\n",
    "    mets = dmets[k]\n",
    "\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(alphas,mets[:,0], line_styles[j])\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.xticks([])\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(alphas,mets[:,1], line_styles[j])\n",
    "    plt.ylabel(\"Prec\")\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(alphas,mets[:,2], line_styles[j])\n",
    "    plt.ylabel(\"Rec\")\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(alphas,mets[:,2], line_styles[j])\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.xticks([])\n",
    "\n",
    "    \n",
    "plt.xlabel(\"alpha\")\n",
    "plt.xticks(alphas)\n",
    "plt.axis([1, alphas[-1], 0.3, 0.8])\n",
    "plt.gca().legend([str(i) for i in K_vals_fixes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((1200, 500))\n",
    "\n",
    "# generate 2 2d grids for the x & y bounds\n",
    "y, x = np.meshgrid(np.linspace(-1, 1200), np.linspace(1, 500))\n",
    "\n",
    "z = (1 - x / 2. + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)\n",
    "# x and y are bounds, so z should be the value *inside* those bounds.\n",
    "# Therefore, remove the last value from the z array.\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('pcolormesh')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "# plt.show()\n",
    "dmets\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15, 0, -1): # <--Delete me\n",
    "    print(i)\n",
    "    pca = sentiment.PCA(min(i, 50))\n",
    "    pca.fit(X_train[:i])\n",
    "    Xtc_train = pca.transform(X_train[:i])\n",
    "    Ytc_train = y_train[:i]\n",
    "    Xtc_test = pca.transform(X_test)\n",
    "    \n",
    "    for k in range(min(i, 50), 0, -1): # <--Delete me\n",
    "        clf = sentiment.KNNClassifier(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSizeRange = range(X_train.shape[0], 0, -120)\n",
    "trainSizeRangeLength = len(trainSizeRange)\n",
    "knnRange = range(2500, 0, -50)\n",
    "knnRangeLength = len(knnRange)\n",
    "\n",
    "print(\"Will have {} trainSize instances\".format(trainSizeRangeLength))\n",
    "print(\"Will have {} knn instances\".format(knnRangeLength))\n",
    "\n",
    "accColorMap = np.zeros(shape=(trainSizeRangeLength, len(knnRange)))\n",
    "precColorMap = np.zeros(shape=(trainSizeRangeLength, len(knnRange)))\n",
    "recColorMap = np.zeros(shape=(trainSizeRangeLength, len(knnRange)))\n",
    "\n",
    "for i in range(0, len(trainSizeRange)):\n",
    "    trainSize = trainSizeRange[i]\n",
    "    pca = sentiment.PCA(min(i, 50))\n",
    "    pca.fit(X_train[:trainSize])\n",
    "    Xtc_train = pca.transform(X_train[:trainSize])\n",
    "    Ytc_train = y_train[:trainSize]\n",
    "    Xtc_test = pca.transform(X_test)\n",
    "    print(\"Finished training PCA train_size = {}\".format(trainSize))\n",
    "\n",
    "    for j in range(0, knnRangeLength):\n",
    "        k = knnRange[j]\n",
    "        if (k <= trainSize):\n",
    "            clf = sentiment.KNNClassifier(k)\n",
    "            clf.fit(Xtc_train, Ytc_train)\n",
    "            preds = clf.predict(Xtc_test)\n",
    "            acc, prec, rec = getMetrics(preds, y_test)\n",
    "            f1 = f1score(prec,rec)\n",
    "            accColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = acc\n",
    "            precColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = prec\n",
    "            recColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = rec\n",
    "            f1ColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = f1\n",
    "            print(\"Finished k = {}, train_size = {}\".format(k, trainSize))\n",
    "        else:\n",
    "            accColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = 0\n",
    "            precColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = 0\n",
    "            recColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = 0\n",
    "            f1ColorMap[trainSizeRangeLength - i - 1, knnRangeLength - j - 1] = 0\n",
    "\n",
    "saveData(\"accColorMap\", accColorMap)\n",
    "saveData(\"precColorMap\", precColorMap)\n",
    "saveData(\"recColorMap\", recColorMap)\n",
    "saveData(\"f1ColorMap\", f1ColorMap)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "c = ax.pcolormesh(accColorMap, cmap='hot')\n",
    "ax.set_title('Train_size vs neighbors')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax.axis([0, accColorMap.shape[1], 0, accColorMap.shape[0]])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "c2 = ax2.pcolormesh(precColorMap, cmap='hot')\n",
    "ax2.set_title('pcolormesh')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax2.axis([0, precColorMap.shape[1], 0, precColorMap.shape[0]])\n",
    "fig2.colorbar(c2, ax=ax2)\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "c3 = ax3.pcolormesh(recColorMap, cmap='hot')\n",
    "ax3.set_title('pcolormesh')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax3.axis([0, recColorMap.shape[1], 0, recColorMap.shape[0]])\n",
    "fig3.colorbar(c3, ax=ax3)\n",
    "\n",
    "fig4, ax4 = plt.subplots()\n",
    "\n",
    "c4 = ax4.pcolormesh(f1ColorMap, cmap='hot')\n",
    "ax4.set_title('pcolormesh')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax4.axis([0, f1ColorMap.shape[1], 0, f1ColorMap.shape[0]])\n",
    "fig4.colorbar(c4, ax=ax4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "c = ax.pcolormesh(accColorMap, cmap='hot', vmin=0, vmax=1)\n",
    "ax.set_title('Train_size vs neighbors (accuracy)')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax.axis([0, accColorMap.shape[1], 0, accColorMap.shape[0]])\n",
    "ax.set_xlabel(\"Train size (1:125)\")\n",
    "ax.set_ylabel(\"K neighbors (1:50)\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "c2 = ax2.pcolormesh(precColorMap, cmap='hot', vmin=0, vmax=1)\n",
    "ax2.set_title('Train_size vs neighbors (precision)')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax2.axis([0, precColorMap.shape[1], 0, precColorMap.shape[0]])\n",
    "ax2.set_xlabel(\"Train size (1:125)\")\n",
    "ax2.set_ylabel(\"K neighbors (1:50)\")\n",
    "fig2.colorbar(c2, ax=ax2)\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "c3 = ax3.pcolormesh(recColorMap, cmap='hot', vmin=0, vmax=1)\n",
    "ax3.set_title('Train_size vs neighbors (recall)')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax3.axis([0, recColorMap.shape[1], 0, recColorMap.shape[0]])\n",
    "ax3.set_xlabel(\"Train size (1:125)\")\n",
    "ax3.set_ylabel(\"K neighbors (1:50)\")\n",
    "fig3.colorbar(c3, ax=ax3)\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "c4 = ax4.pcolormesh(f1ColorMap, cmap='hot', vmin=0, vmax=1)\n",
    "ax4.set_title('Train_size vs neighbors (f1)')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax4.axis([0, f1ColorMap.shape[1], 0, f1ColorMap.shape[0]])\n",
    "ax4.set_xlabel(\"Train size (1:125)\")\n",
    "ax4.set_ylabel(\"K neighbors (1:50)\")\n",
    "fig4.colorbar(c4, ax=ax4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6275, 6424)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-98837f99ec8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;31m#Si alpha>actual_words (o sea no hay tantas palabras como alpha) puede ser que se rompa..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;31m#eso es lo que les pregunt√© por whatsapp, por las dudas pongo este if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mactual_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "#EXPERIMENTO DE MATOOOO, ESCRIBE UN CSV PIOLA, CON VARIAS COLUMNAS PARA ANALIZAR LO QUE UNO QUIERA\n",
    "#PUEDEN DEJAR LAS PRIMERAS TRES VARIABLES FIJAS SI NO QUIEREN EXPERIMENTAR CON EL COUNTVECTORIZER\n",
    "#HAY QUE TOCAR UN POQUITO EL FOR MAS ANIDADO PARA CAMBIAR DE KNN A PCA, PERO SOLO ESO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "\n",
    "ts = calendar.timegm(time.gmtime())\n",
    "name = \"countVect_{}.csv\".format(ts)\n",
    "\n",
    "with open(name, \"w\") as csvFile:  # te hace el open y el close\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(['max_df', 'min_df', 'max_features', 'actual_words', 'acc', 'prec', 'rec', 'f1', 'K', 'alpha', 'timeKNN', 'timePCA'])\n",
    "    #file.write(\"max_df,min_df,max_features,actual_words,acc,prec,rec,f1,K,alpha\\n\")\n",
    "    #si es KNN sin PCA pongo alpha=0 en el csv\n",
    "    \n",
    "    for max_df in [0.20,0.90,0.98]:\n",
    "        for min_df in [0.002, 0.005, 0.01, 0.02, 0.05, 0.1]:\n",
    "                max_features=10000\n",
    "                #no tiene mucho sentido tocarlo al max_features, porque ya estudi√© cuantas palabras quedan \n",
    "                #jugando con max y min, si quiero mas o menos palabras lo hago con max y min\n",
    "                #el maximo en realidad va a ser con min0.002 y es 6500\n",
    "                \n",
    "                vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "                vectorizer.fit(text_train)\n",
    "                X_train, y_train = vectorizer.transform(text_train), (label_train == 'pos').values\n",
    "                X_test, y_test = vectorizer.transform(text_test), (label_test == 'pos').values\n",
    "\n",
    "                #esto te deja ver cuantas palabras quedaron despues del CountVectorizer\n",
    "                actual_words = X_train.shape\n",
    "                print(X_test.shape)\n",
    "                \n",
    "                #es mas feo poner el alpha arriba que el K, pero acelera la experimentacion\n",
    "                for alpha in [0, 10, 50, 200, 600]:  #si es sin PCA es alpha=0\n",
    "                    #Si alpha>actual_words (o sea no hay tantas palabras como alpha) puede ser que se rompa..\n",
    "                    #eso es lo que les pregunt√© por whatsapp, por las dudas pongo este if\n",
    "                    if (alpha>actual_words):\n",
    "                        break\n",
    "                    \n",
    "                    startPCA = time.time()\n",
    "                    \n",
    "                    pca_x_train = np.zeros(0) #la idea ser√≠a que esto nunca se use as√≠, pero si no abajo me dice\n",
    "                    pca_x_test = np.zeros(0) # que no las tengo declaradas\n",
    "                    \n",
    "                    if (not alpha==0): #si es sin PCA\n",
    "                        pca = sentiment.PCA(alpha)\n",
    "                        pca.fit(X_train)\n",
    "                        pca_x_train = pca.transform(X_train)\n",
    "                        pca_x_test = pca.transform(X_test)\n",
    "                    \n",
    "                    endPCA = time.time()\n",
    "                    for K in [25, 250, 450, 550, 800, 1500]:\n",
    "                        startKNN = time.time()\n",
    "                            \n",
    "                        clf = sentiment.KNNClassifier(K)\n",
    "                        \n",
    "                        if (not alpha==0): #si es sin PCA\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                        \n",
    "                        if (alpha==0): #si es con PCA\n",
    "                            clf.fit(pca_x_train, y_train)\n",
    "                            y_pred = clf.predict(pca_x_test)\n",
    "                        \n",
    "                        end = time.time()\n",
    "                        \n",
    "                        acc, prec, rec = getMetrics(y_pred, y_test)\n",
    "                        f1 = f1score(prec,rec)\n",
    "                        timeKNN=(endKNN-startKNN)\n",
    "                        timePCA=(endPCA-startPCA)\n",
    "                        \n",
    "                        writer.writerow([max_df, min_df, max_features, actual_words, acc, prec, rec,  f1, K, alpha, timeKNN, timePCA])\n",
    "                        #file.write(\"{},{},{},{},{},{},{},{}\\n\".format(max_df,min_df,max_features,actual_words,acc,prec,rec,f1))\n",
    "\n",
    "#hasta aca tengo abierto el archivo en el que escribo\n",
    "\n",
    "\n",
    "#max df cambia muy poco, hay muy pocas palabras que aparecen m√°s de 80% de los comentarios por ej\n",
    "#incluso m√°s que en el 20% de los comentarios no hay muchas, igual tiene sentido hacerlo quizas? para sacar molestos.\n",
    "#Seguro no quiero un max_df bajo como 0.20 porque me saca las mejores palabras para comparar..\n",
    "#Probar si poner max_df=0.1 por ej arruina PCA\n",
    "\n",
    "#min_df=0.003 es el momento clave que max_features se acerca a 5000. O sea que hay como 4800 palabras que aparecen en 0.3% comentarios\n",
    "#ya con min_df=0.01% tenemos como 43.000 palabras. Todas las palabras aparecen en al menos 0.01% de comments?? es raro eso\n",
    "#Duh.. 0.01% ya es menos de 1 comentario.. Podr√≠a pensar que el m√°ximo razonable para probar de min_df es 0.2%, que es aparecer\n",
    "#en 12 comentarios aprox, que podemos pensar empieza a ser suficiente para sacar conclusiones.\n",
    "#Eso ya te baja el N¬∫ palabras a 6500, que es bastante cerca del m√°ximo que quer√≠amos tomar de 5000. Experimentar por ah√≠\n",
    "\n",
    "#OJO, count vectorizer solo nos deja 43.000 palabras, que son todas las que hay en text_train, y al text_test lo reduce\n",
    "#a esas palabras!!! o sea que ni cuenta las palabras que hay en los comentarios de test que no estaban en el train\n",
    "\n",
    "#para los valores que ven√≠a por defecto de la catedra (0.9  0.01):\n",
    "#segun esto, los comentarios de train quedan en promedio con 102 palabras, y los de test con 98, no es terrible, pero claramente\n",
    "#les sacamos varias palabras a los de train... igualmente seguro eran las menos frecuentes, porque nunca aparecian en train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
